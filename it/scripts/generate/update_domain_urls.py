#!/usr/bin/env python3
"""
Script pour mettre √† jour toutes les URLs du site avec le domaine d√©fini dans translations.csv.

Ce script :
1. Lit le domaine depuis translations.csv (cl√©: site.domain)
2. Remplace toutes les URLs dans index.html et toutes les pages g√©n√©r√©es
3. Met √† jour les meta tags (Open Graph, Twitter, canonical, hreflang, schema.org)

Usage:
    python3 scripts/generate/update_domain_urls.py
"""

import csv
import re
from pathlib import Path
from urllib.parse import urlparse

BASE_DIR = Path(__file__).parent.parent.parent
TRANSLATIONS_CSV = BASE_DIR / 'translations.csv'
INDEX_HTML = BASE_DIR / 'index.html'

def load_domain_from_csv():
    """Charge le domaine depuis translations.csv (colonne 'en')."""
    if not TRANSLATIONS_CSV.exists():
        print(f"‚ùå Fichier non trouv√©: {TRANSLATIONS_CSV}")
        return None
    
    try:
        with open(TRANSLATIONS_CSV, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                key = row.get('key', '').strip()
                if key == 'site.domain':
                    domain = row.get('en', '').strip()
                    if domain and not domain.startswith('='):
                        # S'assurer que le domaine se termine par / pour la racine
                        if not domain.endswith('/'):
                            domain = domain.rstrip('/')
                        return domain
    except Exception as e:
        print(f"‚ùå Erreur lors de la lecture du CSV: {e}")
        return None
    
    print("‚ö†Ô∏è  Cl√© 'site.domain' non trouv√©e dans translations.csv")
    return None

def extract_base_domain(url):
    """Extrait le domaine de base d'une URL."""
    if not url or not url.startswith('http'):
        return None
    parsed = urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}"

def update_urls_in_content(content, old_domain, new_domain):
    """Remplace toutes les URLs dans le contenu HTML."""
    if not old_domain or not new_domain:
        return content
    
    # Normaliser les domaines (sans / √† la fin pour la base)
    old_base = old_domain.rstrip('/')
    new_base = new_domain.rstrip('/')
    
    # Extraire le nom de domaine (sans protocole) pour les remplacements
    old_domain_name = old_base.replace('http://', '').replace('https://', '')
    new_domain_name = new_base.replace('http://', '').replace('https://', '')
    
    # Remplacer seulement les URLs compl√®tes (pas les attributs xmlns, etc.)
    # Pattern 1: URLs compl√®tes dans les attributs href="..." et content="..."
    content = re.sub(
        rf'(href|content)="https?://{re.escape(old_domain_name)}([^"]*)"',
        rf'\1="{new_base}\2"',
        content
    )
    
    # Pattern 2: URLs dans les balises <link> et <meta>
    content = re.sub(
        rf'(<link[^>]+(?:href|rel)=")https?://{re.escape(old_domain_name)}([^"]*)"',
        rf'\1{new_base}\2"',
        content
    )
    
    # Pattern 3: URLs dans JSON-LD schema.org
    content = re.sub(
        rf'"url":\s*"https?://{re.escape(old_domain_name)}([^"]*)"',
        f'"url": "{new_base}\\1"',
        content
    )
    
    # Pattern 4: URLs dans les balises hreflang
    content = re.sub(
        rf'(<link[^>]+hreflang="[^"]+"[^>]+href=")https?://{re.escape(old_domain_name)}([^"]*)"',
        rf'\1{new_base}\2"',
        content
    )
    
    # Pattern 5: URLs compl√®tes standalone (pas dans les attributs)
    # Seulement si c'est une URL compl√®te avec chemin
    content = re.sub(
        rf'https?://{re.escape(old_domain_name)}(/[^\s"\'<>]*)',
        rf'{new_base}\1',
        content
    )
    
    return content

def find_old_domain_in_content(content):
    """Trouve l'ancien domaine dans le contenu."""
    # Chercher les patterns connus
    known_patterns = [
        r'https?://votresite\.com',
        r'https?://localhost:\d+',
        r'https?://makita-shop\.pages\.dev',
    ]
    
    for pattern in known_patterns:
        match = re.search(pattern, content)
        if match:
            return match.group(0)
    
    # Chercher n'importe quelle URL
    url_match = re.search(r'https?://[^\s"\'<>/]+', content)
    if url_match:
        url = url_match.group(0)
        # V√©rifier que ce n'est pas un lien externe (aliexpress, google, etc.)
        if 'aliexpress.com' not in url and 'google.com' not in url and 'policies.google.com' not in url:
            return extract_base_domain(url)
    
    return None

def update_index_html(new_domain):
    """Met √† jour toutes les URLs dans index.html."""
    if not INDEX_HTML.exists():
        print(f"‚ùå Fichier non trouv√©: {INDEX_HTML}")
        return False
    
    try:
        content = INDEX_HTML.read_text(encoding='utf-8')
        original_content = content
        
        # Trouver l'ancien domaine
        old_domain = find_old_domain_in_content(content)
        
        if old_domain:
            print(f"  üîÑ Remplacement: {old_domain} ‚Üí {new_domain}")
            content = update_urls_in_content(content, old_domain, new_domain)
        else:
            print(f"  ‚ö†Ô∏è  Aucun ancien domaine d√©tect√©, v√©rification manuelle recommand√©e")
        
        # Sauvegarder seulement si chang√©
        if content != original_content:
            INDEX_HTML.write_text(content, encoding='utf-8')
            print(f"  ‚úÖ index.html mis √† jour")
        else:
            print(f"  ‚ÑπÔ∏è  index.html d√©j√† √† jour")
        return True
    except Exception as e:
        print(f"  ‚ùå Erreur lors de la mise √† jour de index.html: {e}")
        return False

def update_generated_pages(new_domain):
    """Met √† jour toutes les URLs dans les pages g√©n√©r√©es."""
    pages_dirs = [
        BASE_DIR / 'page_html' / 'categories',
        BASE_DIR / 'page_html' / 'products',
        BASE_DIR / 'page_html' / 'legal',
    ]
    
    updated_count = 0
    
    for pages_dir in pages_dirs:
        if not pages_dir.exists():
            continue
        
        html_files = list(pages_dir.rglob('*.html'))
        for html_file in html_files:
            try:
                content = html_file.read_text(encoding='utf-8')
                original_content = content
                
                # Trouver l'ancien domaine
                old_domain = find_old_domain_in_content(content)
                
                if old_domain and old_domain != new_domain:
                    content = update_urls_in_content(content, old_domain, new_domain)
                
                if content != original_content:
                    html_file.write_text(content, encoding='utf-8')
                    updated_count += 1
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Erreur avec {html_file.name}: {e}")
    
    if updated_count > 0:
        print(f"  ‚úÖ {updated_count} page(s) g√©n√©r√©e(s) mise(s) √† jour")
    else:
        print(f"  ‚ÑπÔ∏è  Aucune page g√©n√©r√©e √† mettre √† jour")
    
    return updated_count

def update_sitemap(new_domain):
    """Met √† jour les URLs dans le sitemap.xml et sitemap.html."""
    sitemap_xml = BASE_DIR / 'sitemap.xml'
    sitemap_html = BASE_DIR / 'sitemap.html'
    
    updated = False
    
    # Mettre √† jour sitemap.xml
    if sitemap_xml.exists():
        try:
            content = sitemap_xml.read_text(encoding='utf-8')
            original_content = content
            
            old_domain = find_old_domain_in_content(content)
            if old_domain and old_domain != new_domain:
                content = update_urls_in_content(content, old_domain, new_domain)
            
            if content != original_content:
                sitemap_xml.write_text(content, encoding='utf-8')
                print(f"  ‚úÖ sitemap.xml mis √† jour")
                updated = True
            else:
                print(f"  ‚ÑπÔ∏è  sitemap.xml d√©j√† √† jour")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Erreur avec sitemap.xml: {e}")
    else:
        print(f"  ‚ÑπÔ∏è  sitemap.xml non trouv√© (optionnel)")
    
    # Mettre √† jour sitemap.html
    if sitemap_html.exists():
        try:
            content = sitemap_html.read_text(encoding='utf-8')
            original_content = content
            
            old_domain = find_old_domain_in_content(content)
            if old_domain and old_domain != new_domain:
                content = update_urls_in_content(content, old_domain, new_domain)
            
            if content != original_content:
                sitemap_html.write_text(content, encoding='utf-8')
                print(f"  ‚úÖ sitemap.html mis √† jour")
                updated = True
            else:
                print(f"  ‚ÑπÔ∏è  sitemap.html d√©j√† √† jour")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Erreur avec sitemap.html: {e}")
    else:
        print(f"  ‚ÑπÔ∏è  sitemap.html non trouv√© (optionnel)")
    
    return updated

def main():
    """Fonction principale."""
    print("=" * 70)
    print("üåê MISE √Ä JOUR DES URLs AVEC LE DOMAINE DU CSV")
    print("=" * 70)
    print()
    
    # 1. Charger le domaine depuis le CSV
    print("üìñ Chargement du domaine depuis translations.csv...")
    new_domain = load_domain_from_csv()
    
    if not new_domain:
        print("‚ùå Impossible de charger le domaine. V√©rifiez que 'site.domain' existe dans translations.csv")
        return False
    
    print(f"‚úÖ Domaine trouv√©: {new_domain}")
    print()
    
    # 2. Mettre √† jour index.html
    print("üìÑ Mise √† jour de index.html...")
    update_index_html(new_domain)
    print()
    
    # 3. Mettre √† jour les pages g√©n√©r√©es
    print("üìÑ Mise √† jour des pages g√©n√©r√©es...")
    update_generated_pages(new_domain)
    print()
    
    # 4. Mettre √† jour le sitemap
    print("üó∫Ô∏è  Mise √† jour du sitemap...")
    update_sitemap(new_domain)
    print()
    
    print("=" * 70)
    print("‚úÖ TERMIN√â!")
    print("=" * 70)
    print()
    print(f"üí° Toutes les URLs ont √©t√© mises √† jour avec: {new_domain}")
    print("üí° Pour changer le domaine, modifiez 'site.domain' dans translations.csv et relancez ce script.")
    print()
    
    return True

if __name__ == "__main__":
    main()


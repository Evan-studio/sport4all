#!/usr/bin/env python3
"""
Script pour g√©n√©rer automatiquement les sitemaps pour tous les sites multilingues.

Ce script :
1. D√©tecte automatiquement tous les dossiers de langue (fr/, de/, es/, etc.)
2. G√©n√®re un sitemap sp√©cifique pour chaque langue avec les bonnes URLs
3. G√©n√®re un sitemap index √† la racine qui r√©f√©rence tous les sitemaps de langue
4. Met √† jour automatiquement quand on ajoute une nouvelle langue

Usage:
    python3 generate_sitemaps.py
"""

import csv
import re
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse

BASE_DIR = Path(__file__).parent

# Dossiers √† exclure lors de la d√©tection des langues
EXCLUDED_DIRS = {
    'APPLI:SCRIPT aliexpress', 'scripts', 'config', 'images', 'page_html', 
    'upload_cloudflare', 'sauv', 'CSV', '__pycache__', '.git', 'node_modules'
}

def find_language_directories():
    """Trouve automatiquement tous les dossiers de langue."""
    lang_dirs = []
    for item in BASE_DIR.iterdir():
        if (item.is_dir() and 
            not item.name.startswith('.') and 
            item.name not in EXCLUDED_DIRS and
            (item / 'index.html').exists() and 
            (item / 'translations.csv').exists()):
            lang_dirs.append(item)
    return sorted(lang_dirs, key=lambda x: x.name.lower())

def load_domain_from_csv(lang_dir):
    """Charge le domaine depuis translations.csv d'une langue."""
    translations_csv = lang_dir / 'translations.csv'
    if not translations_csv.exists():
        return None
    
    try:
        with open(translations_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                key = row.get('key', '').strip()
                if key == 'site.domain':
                    # Chercher dans toutes les colonnes possibles
                    domain = None
                    for col in row.keys():
                        if col != 'key' and col != 'description':
                            value = row.get(col, '').strip()
                            if value and not value.startswith('=') and not value.startswith('#'):
                                domain = value
                                break
                    
                    if domain:
                        # Nettoyer le domaine
                        domain = domain.rstrip('/')
                        if not domain.startswith('http'):
                            domain = f'https://{domain}'
                        return domain
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Erreur lors de la lecture du CSV: {e}")
    
    return None

def get_base_domain():
    """Trouve le domaine de base en cherchant dans tous les dossiers de langue."""
    lang_dirs = find_language_directories()
    for lang_dir in lang_dirs:
        domain = load_domain_from_csv(lang_dir)
        if domain:
            return domain
    
    # Fallback : chercher dans translations.csv √† la racine
    root_translations = BASE_DIR / 'translations.csv'
    if root_translations.exists():
        try:
            with open(root_translations, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    key = row.get('key', '').strip()
                    if key == 'site.domain':
                        for col in row.keys():
                            if col != 'key' and col != 'description':
                                value = row.get(col, '').strip()
                                if value and not value.startswith('=') and not value.startswith('#'):
                                    domain = value.rstrip('/')
                                    if not domain.startswith('http'):
                                        domain = f'https://{domain}'
                                    return domain
        except:
            pass
    
    # Fallback par d√©faut
    return 'https://www.senseofthailand.com'

def get_lastmod_date(file_path):
    """R√©cup√®re la date de modification d'un fichier."""
    if file_path.exists():
        mtime = file_path.stat().st_mtime
        return datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
    return datetime.now().strftime('%Y-%m-%d')

def find_html_pages(lang_dir, lang_code):
    """Trouve toutes les pages HTML d'une langue."""
    pages = []
    base_domain = get_base_domain()
    
    # Index de la langue
    index_file = lang_dir / 'index.html'
    if index_file.exists():
        pages.append({
            'url': f'{base_domain}/{lang_code}/',
            'lastmod': get_lastmod_date(index_file),
            'priority': '1.0',
            'changefreq': 'daily'
        })
    
    # Pages cat√©gories
    categories_dir = lang_dir / 'page_html' / 'categories'
    if categories_dir.exists():
        for html_file in sorted(categories_dir.glob('*.html')):
            if html_file.name != 'index.html':  # Exclure les index.html dans les cat√©gories
                pages.append({
                    'url': f'{base_domain}/{lang_code}/page_html/categories/{html_file.name}',
                    'lastmod': get_lastmod_date(html_file),
                    'priority': '0.8',
                    'changefreq': 'weekly'
                })
    
    # Pages produits
    products_dir = lang_dir / 'page_html' / 'products'
    if products_dir.exists():
        for html_file in sorted(products_dir.glob('produit-*.html')):
            pages.append({
                'url': f'{base_domain}/{lang_code}/page_html/products/{html_file.name}',
                'lastmod': get_lastmod_date(html_file),
                'priority': '0.7',
                'changefreq': 'monthly'
            })
    
    # Pages l√©gales
    legal_dir = lang_dir / 'page_html' / 'legal'
    if legal_dir.exists():
        for html_file in sorted(legal_dir.glob('*.html')):
            pages.append({
                'url': f'{base_domain}/{lang_code}/page_html/legal/{html_file.name}',
                'lastmod': get_lastmod_date(html_file),
                'priority': '0.5',
                'changefreq': 'monthly'
            })
    
    return pages

def generate_language_sitemap(lang_dir, lang_code):
    """G√©n√®re un sitemap XML pour une langue sp√©cifique."""
    pages = find_html_pages(lang_dir, lang_code)
    
    if not pages:
        print(f"  ‚ö†Ô∏è  Aucune page trouv√©e pour {lang_code}")
        return None
    
    sitemap_content = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
    
    for page in pages:
        sitemap_content.append('  <url>')
        sitemap_content.append(f'    <loc>{page["url"]}</loc>')
        sitemap_content.append(f'    <lastmod>{page["lastmod"]}</lastmod>')
        sitemap_content.append(f'    <changefreq>{page["changefreq"]}</changefreq>')
        sitemap_content.append(f'    <priority>{page["priority"]}</priority>')
        sitemap_content.append('  </url>')
    
    sitemap_content.append('</urlset>')
    
    return '\n'.join(sitemap_content)

def generate_sitemap_index(lang_codes):
    """G√©n√®re le sitemap index qui r√©f√©rence tous les sitemaps de langue."""
    base_domain = get_base_domain()
    today = datetime.now().strftime('%Y-%m-%d')
    
    sitemap_content = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap_content.append('<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
    
    # Trier les codes de langue pour avoir 'en' en premier
    sorted_lang_codes = sorted(lang_codes, key=lambda x: (x != 'en', x))
    
    for lang_code in sorted_lang_codes:
        sitemap_url = f'{base_domain}/sitemap-{lang_code}.xml'
        sitemap_content.append('  <sitemap>')
        sitemap_content.append(f'    <loc>{sitemap_url}</loc>')
        sitemap_content.append(f'    <lastmod>{today}</lastmod>')
        sitemap_content.append('  </sitemap>')
    
    sitemap_content.append('</sitemapindex>')
    
    return '\n'.join(sitemap_content)

def cleanup_old_sitemaps():
    """Supprime tous les anciens fichiers sitemap avant de g√©n√©rer les nouveaux."""
    deleted_count = 0
    
    # Supprimer les sitemaps √† la racine
    for sitemap_file in BASE_DIR.glob('sitemap*.xml'):
        try:
            sitemap_file.unlink()
            deleted_count += 1
            print(f"  üóëÔ∏è  Supprim√©: {sitemap_file.name}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Erreur lors de la suppression de {sitemap_file.name}: {e}")
    
    # Supprimer les sitemaps dans les dossiers de langue
    for item in BASE_DIR.iterdir():
        if item.is_dir() and not item.name.startswith('.') and item.name not in EXCLUDED_DIRS:
            for sitemap_file in item.glob('sitemap*.xml'):
                try:
                    sitemap_file.unlink()
                    deleted_count += 1
                    print(f"  üóëÔ∏è  Supprim√©: {item.name}/{sitemap_file.name}")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Erreur lors de la suppression de {item.name}/{sitemap_file.name}: {e}")
    
    return deleted_count

def main():
    """Fonction principale."""
    print("=" * 70)
    print("üó∫Ô∏è  G√âN√âRATION DES SITEMAPS MULTILINGUES")
    print("=" * 70)
    print()
    
    # 0. Nettoyer les anciens sitemaps
    print("üßπ Nettoyage des anciens sitemaps...")
    deleted_count = cleanup_old_sitemaps()
    if deleted_count > 0:
        print(f"  ‚úÖ {deleted_count} ancien(s) sitemap(s) supprim√©(s)")
    else:
        print("  ‚ÑπÔ∏è  Aucun ancien sitemap trouv√©")
    print()
    
    # 1. D√©tecter tous les dossiers de langue
    print("üîç D√©tection des sites de langue...")
    lang_dirs = find_language_directories()
    
    if not lang_dirs:
        print("‚ùå Aucun dossier de langue trouv√©")
        print("   Assurez-vous que chaque langue a un dossier avec index.html et translations.csv")
        return False
    
    print(f"‚úÖ {len(lang_dirs)} site(s) de langue d√©tect√©(s):")
    for lang_dir in lang_dirs:
        print(f"   - {lang_dir.name}/")
    print()
    
    # 2. R√©cup√©rer le domaine
    base_domain = get_base_domain()
    print(f"üåê Domaine d√©tect√©: {base_domain}")
    print()
    
    # 3. G√©n√©rer un sitemap pour la racine (en) si elle existe
    generated_sitemaps = []
    root_index = BASE_DIR / 'index.html'
    root_translations = BASE_DIR / 'translations.csv'
    
    if root_index.exists() and root_translations.exists():
        print("üìù G√©n√©ration des sitemaps par langue...")
        print("-" * 70)
        print(f"\nüìÑ G√©n√©ration de sitemap-en.xml (racine)...")
        
        # G√©n√©rer le sitemap pour la racine (en)
        root_pages = []
        base_domain = get_base_domain()
        
        # Index de la racine
        root_pages.append({
            'url': f'{base_domain}/',
            'lastmod': get_lastmod_date(root_index),
            'priority': '1.0',
            'changefreq': 'daily'
        })
        
        # Pages cat√©gories de la racine
        root_categories_dir = BASE_DIR / 'page_html' / 'categories'
        if root_categories_dir.exists():
            for html_file in sorted(root_categories_dir.glob('*.html')):
                if html_file.name != 'index.html':
                    root_pages.append({
                        'url': f'{base_domain}/page_html/categories/{html_file.name}',
                        'lastmod': get_lastmod_date(html_file),
                        'priority': '0.8',
                        'changefreq': 'weekly'
                    })
        
        # Pages produits de la racine
        root_products_dir = BASE_DIR / 'page_html' / 'products'
        if root_products_dir.exists():
            for html_file in sorted(root_products_dir.glob('produit-*.html')):
                root_pages.append({
                    'url': f'{base_domain}/page_html/products/{html_file.name}',
                    'lastmod': get_lastmod_date(html_file),
                    'priority': '0.7',
                    'changefreq': 'monthly'
                })
        
        # Pages l√©gales de la racine
        root_legal_dir = BASE_DIR / 'page_html' / 'legal'
        if root_legal_dir.exists():
            for html_file in sorted(root_legal_dir.glob('*.html')):
                root_pages.append({
                    'url': f'{base_domain}/page_html/legal/{html_file.name}',
                    'lastmod': get_lastmod_date(html_file),
                    'priority': '0.5',
                    'changefreq': 'monthly'
                })
        
        if root_pages:
            sitemap_content = ['<?xml version="1.0" encoding="UTF-8"?>']
            sitemap_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
            
            for page in root_pages:
                sitemap_content.append('  <url>')
                sitemap_content.append(f'    <loc>{page["url"]}</loc>')
                sitemap_content.append(f'    <lastmod>{page["lastmod"]}</lastmod>')
                sitemap_content.append(f'    <changefreq>{page["changefreq"]}</changefreq>')
                sitemap_content.append(f'    <priority>{page["priority"]}</priority>')
                sitemap_content.append('  </url>')
            
            sitemap_content.append('</urlset>')
            sitemap_en_content = '\n'.join(sitemap_content)
            
            sitemap_file = BASE_DIR / 'sitemap-en.xml'
            sitemap_file.write_text(sitemap_en_content, encoding='utf-8')
            
            page_count = sitemap_en_content.count('<url>')
            print(f"  ‚úÖ {page_count} page(s) ajout√©e(s)")
            print(f"  üìÅ Fichier: {sitemap_file.name} (racine)")
            generated_sitemaps.append('en')
        else:
            print(f"  ‚ö†Ô∏è  Aucune page trouv√©e pour la racine")
    else:
        print("üìù G√©n√©ration des sitemaps par langue...")
        print("-" * 70)
    
    # 4. G√©n√©rer un sitemap pour chaque langue
    for lang_dir in lang_dirs:
        lang_code = lang_dir.name.lower()
        print(f"\nüìÑ G√©n√©ration de sitemap-{lang_code}.xml...")
        
        sitemap_content = generate_language_sitemap(lang_dir, lang_code)
        
        if sitemap_content:
            # Sauvegarder le sitemap √† la racine (pour Google Search Console)
            sitemap_file = BASE_DIR / f'sitemap-{lang_code}.xml'
            sitemap_file.write_text(sitemap_content, encoding='utf-8')
            
            # Compter les pages
            page_count = sitemap_content.count('<url>')
            print(f"  ‚úÖ {page_count} page(s) ajout√©e(s)")
            print(f"  üìÅ Fichier: {sitemap_file.name} (racine)")
            generated_sitemaps.append(lang_code)
        else:
            print(f"  ‚ö†Ô∏è  Aucune page trouv√©e, sitemap non g√©n√©r√©")
    
    print()
    print("-" * 70)
    
    # 5. G√©n√©rer le sitemap index √† la racine
    print("\nüìã G√©n√©ration du sitemap index (racine)...")
    sitemap_index_content = generate_sitemap_index(generated_sitemaps)
    
    sitemap_index_file = BASE_DIR / 'sitemap.xml'
    sitemap_index_file.write_text(sitemap_index_content, encoding='utf-8')
    
    print(f"  ‚úÖ Sitemap index g√©n√©r√© avec {len(generated_sitemaps)} langue(s)")
    print(f"  üìÅ Fichier: {sitemap_index_file}")
    print()
    
    # 6. G√©n√©rer aussi un sitemap combin√© (toutes les URLs dans un seul fichier)
    # Utile si Google a des probl√®mes avec le sitemap index
    print("üìã G√©n√©ration du sitemap combin√© (sitemap-all.xml)...")
    all_pages = []
    
    # Ajouter les pages de la racine (en) si elles existent
    if 'en' in generated_sitemaps:
        root_index = BASE_DIR / 'index.html'
        if root_index.exists():
            all_pages.append({
                'url': f'{base_domain}/',
                'lastmod': get_lastmod_date(root_index),
                'priority': '1.0',
                'changefreq': 'daily'
            })
        
        root_categories_dir = BASE_DIR / 'page_html' / 'categories'
        if root_categories_dir.exists():
            for html_file in sorted(root_categories_dir.glob('*.html')):
                if html_file.name != 'index.html':
                    all_pages.append({
                        'url': f'{base_domain}/page_html/categories/{html_file.name}',
                        'lastmod': get_lastmod_date(html_file),
                        'priority': '0.8',
                        'changefreq': 'weekly'
                    })
        
        root_products_dir = BASE_DIR / 'page_html' / 'products'
        if root_products_dir.exists():
            for html_file in sorted(root_products_dir.glob('produit-*.html')):
                all_pages.append({
                    'url': f'{base_domain}/page_html/products/{html_file.name}',
                    'lastmod': get_lastmod_date(html_file),
                    'priority': '0.7',
                    'changefreq': 'monthly'
                })
    
    # Ajouter les pages de chaque langue
    for lang_dir in lang_dirs:
        lang_code = lang_dir.name.lower()
        lang_pages = find_html_pages(lang_dir, lang_code)
        all_pages.extend(lang_pages)
    
    # G√©n√©rer le sitemap combin√© si on a moins de 50000 URLs (limite Google)
    if len(all_pages) > 0 and len(all_pages) < 50000:
        sitemap_all_content = ['<?xml version="1.0" encoding="UTF-8"?>']
        sitemap_all_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
        
        for page in all_pages:
            sitemap_all_content.append('  <url>')
            sitemap_all_content.append(f'    <loc>{page["url"]}</loc>')
            sitemap_all_content.append(f'    <lastmod>{page["lastmod"]}</lastmod>')
            sitemap_all_content.append(f'    <changefreq>{page["changefreq"]}</changefreq>')
            sitemap_all_content.append(f'    <priority>{page["priority"]}</priority>')
            sitemap_all_content.append('  </url>')
        
        sitemap_all_content.append('</urlset>')
        sitemap_all_file = BASE_DIR / 'sitemap-all.xml'
        sitemap_all_file.write_text('\n'.join(sitemap_all_content), encoding='utf-8')
        
        print(f"  ‚úÖ Sitemap combin√© g√©n√©r√© avec {len(all_pages)} URL(s)")
        print(f"  üìÅ Fichier: {sitemap_all_file.name}")
        print()
        print("  üí° Si Google a des probl√®mes avec sitemap.xml, essayez de soumettre sitemap-all.xml")
    else:
        print(f"  ‚ö†Ô∏è  Sitemap combin√© non g√©n√©r√© ({len(all_pages)} URLs, limite: 50000)")
    print()
    
    # 5. R√©sum√©
    print("=" * 70)
    print("‚úÖ TERMIN√â!")
    print("=" * 70)
    print()
    print("üìä R√©sum√©:")
    print(f"   - Sitemap index: sitemap.xml (racine)")
    for lang_code in generated_sitemaps:
        print(f"   - Sitemap {lang_code}: {lang_code}/sitemap-{lang_code}.xml")
    print()
    print("üí° Pour Google Search Console:")
    print(f"   1. Soumettez uniquement: {base_domain}/sitemap.xml")
    print("   2. Le sitemap index r√©f√©rence automatiquement tous les sitemaps de langue")
    print("   3. Quand vous ajoutez une nouvelle langue, relancez ce script")
    print()
    
    return True

if __name__ == '__main__':
    main()

